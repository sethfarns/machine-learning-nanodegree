{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "\n",
    "\n",
    "* Learn the best hypothesis given data and some domain knowledge\n",
    "* Learn the most probable hypothesis given data and some domain knowledge\n",
    "\n",
    "Pr(h|D) probability of some hypothesis 'h' given some data 'D'\n",
    "\n",
    "Bayes' Rule:\n",
    "~ Pr(h|D) = (Pr(D|h) * Pr(h)) / Pr(D)\n",
    "\n",
    "chain rule:\n",
    "Pr(a,b) = Pr(a|b) * Pr(b)\n",
    "is the same as\n",
    "Pr(a,b) = Pr(b|a) * Pr(a)\n",
    "\n",
    "Pr(D) =  prior belief/probability of seeing some particular sort of data\n",
    "Pr(D|h) = likelihood that we would see some data (or a particular label) given some hypothesis h is true\n",
    "\n",
    "D = {(x$_i$, d$_i$)} (set of data and labels--labels are what we care about)\n",
    "\n",
    "h(x) = {x >= 10}\n",
    "\n",
    "\n",
    "if x = 7\n",
    "\n",
    "then\n",
    "Pr(D|h) = true is 0\n",
    "Pr(D|h) = false is 1\n",
    "\n",
    "Pr(h) is prior on h (this is all our domain knowledge in Bayesian learning)\n",
    "\n",
    "domain knowledge (prior probability that h is true, for all data, as opposed to just for that data point)\n",
    "\n",
    "what could make Pr(h|D) go up?\n",
    "\n",
    "1. a higher Pr(h)\n",
    "2. higher Pr(D|h) (more accurate hypothesis, more successfully predicting labels)\n",
    "3. lower Pr(D) (not connected to the hypothesis directly, typically can be ignored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00784"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bayes_rule(true_pos_prob, true_neg_prob, prior_prob, test_result):\n",
    "    if test_result == True:\n",
    "        return true_pos_prob * prior_prob\n",
    "    else:\n",
    "        return true_neg_prob * prior_prob\n",
    "    \n",
    "bayes_rule(true_pos_prob=0.98, true_neg_prob=0.97, prior_prob=0.008, test_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Learning\n",
    "\n",
    "For each h <- H<br>\n",
    "calculate Pr(h|D) = (Pr(D|h) * Pr(h)) / Pr(D)<br>\n",
    "output:<br>\n",
    "h = argmax of h Pr(h|D)\n",
    "\n",
    "maximum a posteriori (map) takes into account Pr(h)\n",
    "\n",
    "maximum likelihood (ml) hypothesis 'drops' Pr(h)\n",
    ">but really, we have a uniform prior (all hypotheses are equally likely) this ones not practical generally, unless the number of hypotheses is really small\n",
    "\n",
    "if d$_i$ = K * x$_i$ ~Pr(1/2$^k$),\n",
    "then solving Pr(D|h) means identifying K and plugging it into the above probability function (just the Pr part) for each d\n",
    "\n",
    "so if x == 1 and d == 5, Pr(D|h) = Pr(1/2$^5$) = 1/32\n",
    "\n",
    "then get the product of all examples of d\n",
    "\n",
    "### Return to Bayesian Learning\n",
    "\n",
    "Given: {[x$_i$, d$_i$]}<br>\n",
    "d$_i$ = f(x$_i$) + e$_i$ where e is the error and f is the function we're trying to find; e is a Gaussian<br>\n",
    "e$_i$ ~ N(O, $\\sigma$$^2$)\n",
    "\n",
    "maximum likelihood hypothesis:<br>\n",
    "h$_m$$_l$ = argmin $\\Sigma$$_i$ (d$_i$ - h(x$_i$))$^2$ = sum of squared error!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_of_squared_errors(x, y):\n",
    "    # compute difference of y from x for each value\n",
    "    z = [i-j for i,j in zip(x,y)]\n",
    "    # return the sum of the squares in z\n",
    "    return np.sum([i**2 for i in z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "19.4444444444\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "d = [1, 0, 5, 2, 1, 4]\n",
    "x = [1, 3, 6, 10, 11, 13]\n",
    "\n",
    "h1 = [i % 9 for i in x]\n",
    "h2 = [i/3 for i in x]\n",
    "h3 = [2 for i in x]\n",
    "\n",
    "print(sum_of_squared_errors(d, h1))\n",
    "print(sum_of_squared_errors(d, h2))\n",
    "print(sum_of_squared_errors(d, h3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Description Length\n",
    "The best hypothesis (with the maximum a posteriori) is the one that minimizes error and the size/length of your hypothesis; simplest hypothesis that minimizes your error (Occum's razor)\n",
    "\n",
    "### Bayesian Classification\n",
    "If you have three hypotheses, one gives a '+' label with a score of 0.4, the other two both give '-' labels with scores of 0.3, the best label for x is '-', because each hypothesis basically votes, and the '-' label will have a score of 0.6 out of 1.0.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* Bayes rule; swap causes and effects\n",
    "* priors matter\n",
    "* maximum a posteriori (map) hypothesis, maximum likelihood (the map you get when the prior is uniform)\n",
    "* connected map and least squares\n",
    "* classification: voting of hypotheses, bayes optimal classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
