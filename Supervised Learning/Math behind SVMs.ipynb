{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "You want a line that leaves as much space as possible from the boundaries\n",
    "\n",
    "\n",
    "y = w$^T$x + b for hyperplanes\n",
    "\n",
    "y: classification label\n",
    "w: parameters of the plane\n",
    "b: moves the plane in and out of the origin\n",
    "x: input/features\n",
    "\n",
    "w$^T$x + b = 0 (equation for decision boundary; above is positive, below is negative)\n",
    "\n",
    "y <- {-1, 1}\n",
    "\n",
    "w$^T$x + b = 1 for top boundary\n",
    "\n",
    "w$^T$x + b = -1 for bottom boundary\n",
    "\n",
    "pick a point on each boundary to determine distance between them<br>\n",
    "w$^T$x$_1$ + b = 1<br>\n",
    "w$^T$x$_2$ + b = -1<br>\n",
    "\n",
    "Just subtract the two lines from each other to get a single equation that represents the difference between them\n",
    "\n",
    "w$^T$(x$_1$ - x$_2$) = 2\n",
    "\n",
    "Divide both sides by length of w, gives you m (or margin) = 2\n",
    "\n",
    "(w$^T$(x$_1$ - x$_2$))/||w|| = 2/||w||\n",
    "\n",
    "m = 2/||w||\n",
    "\n",
    "We want to maximize m while classifying everything correctly\n",
    "\n",
    "classifying everything correctly = y$_i$(w$^T$x$_i$ + b) >= 1\n",
    "\n",
    "instead minimize 1/2 * ||w||$^2$\n",
    "\n",
    "can turn into a quadratic programming problem\n",
    "\n",
    "only takes a few vectors (ones with non-zero alphas) to provide all the support for v (these are the support vectors)\n",
    "\n",
    "### Linearly Married\n",
    "\n",
    "What to do when data is not perfectly linearly separable?\n",
    "\n",
    "A few outliers:\n",
    "Find line that makes minimal amount of errors while also maximizing margin if you're allowed to flip a few points from positive to negative and negative to positive\n",
    "\n",
    "Linearly married:\n",
    "e.g. minuses surrounding positives\n",
    "\n",
    "turn [q$_1$, q$_2$] into\n",
    "[q$_1^2$, q$_2^2$, $\\sqrt{2}$ * q$_1$ * q$_2$]\n",
    "\n",
    "can convert linear relationship into something that looks like a circle through these transformations; more about distance than the direction you're pointing\n",
    "\n",
    "aka 'the kernel trick'\n",
    "we care about maximizing some function that depends highly on how different data points are alike or different; can use different function to substitute as notion of similarity\n",
    "\n",
    "project into higher dimensional space where points are linearly separable\n",
    "\n",
    "### Kernel\n",
    "\n",
    "inject domain knowledge into SVM algorithm via kernel function\n",
    "project into higher dimensional space where points are linearly separable\n",
    "with the domain knowledge, we don't actually have to project our points ('the kernel trick')\n",
    "\n",
    "polynomial kernel:\n",
    "\n",
    "k = (x$^T$y + c)$^p$ c: constant, p: power\n",
    "\n",
    "allows us to represent polynomial functions\n",
    "\n",
    "tons and tons of kernel functions you can use\n",
    "\n",
    "kernel function captures your domain knowledge about what it means to be similar in your dataset\n",
    "\n",
    "even strings--maybe your data points are similar if their edit distance is small, e.g. cat and scat are more similar than cat and dog\n",
    "\n",
    "Mercer Condition: it acts like a distance or a similarity\n",
    "\n",
    "### Summary\n",
    "\n",
    "* margins are important for generalization & avoiding overfitting\n",
    "* find linear separator with largest margin (maximize margins)\n",
    "* optimization problem for finding max margins: Quadratic Problems\n",
    "* support vectors are the vectors used for defining the margin\n",
    "* SVMs are eager lazy learners (only use what's necessary)\n",
    "* use kernel tricks to represent domain knowledge\n",
    "* kernels should satisfy Mercer condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing Around with Kernel Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "several common kernel functions exist in the box, but you can write a custom kernel as well\n",
    "\n",
    "```python\n",
    "['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "```\n",
    "\n",
    "#### C and gamma\n",
    "\n",
    "C controls tradeoff between smooth decision boundary and one that classifies all the training points correctly\n",
    "\n",
    "larger C tries to get more training points correct, smaller C tries to get larger decision boundary (more generalized solution)\n",
    "\n",
    "gamma defines how far the influence of a single training example reaches\n",
    "\n",
    "low values = far reach, even far away support vectors taken into consideration\n",
    "high values = close reach, only takes into account support vectors nearest the boundary\n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "Tuning hyperparameters (C, gamma, kernel)\n",
    "\n",
    "#### Strengths and weaknesses\n",
    "\n",
    "Struggle with lots of noise, lots of features, very large dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
