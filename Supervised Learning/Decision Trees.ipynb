{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs. Regression\n",
    "\n",
    "Classification: mapping input to discrete label (or labels)\n",
    "Regression: continuous value functions\n",
    "\n",
    "### Classification Learning\n",
    "\n",
    "<h5>definition of terms:</h5>\n",
    "* instances: vectors of attributes that define your input space\n",
    "* concept: function to map inputs to outputs\n",
    "* target concept: the function (answer) that we're trying to find\n",
    "* hypothesis class: set of all concepts that you're willing to entertain\n",
    "* sample (training set): set of inputs paired with labels which are the correct output\n",
    "* candidate: concept that you think might be the target concept\n",
    "* testing set: set of inputs paired with labels, reserved for testing to judge candidates\n",
    "\n",
    "### Representation\n",
    "\n",
    "Attributes or features to describe our instances, which will help determine the classification\n",
    "(i.e. for whether or not to eat at a restaurant, features could be restaurant class, average price, type of food, atmosphere, parking, etc.)\n",
    "\n",
    "Pick particular attribute (node), ask question about it; answer is the edge (specific values). Making decisions by asking series of questions.\n",
    "\n",
    "### Expressiveness: AND, OR, XOR\n",
    "\n",
    "For 'any' ('n' OR's), size of tree is linear on n\n",
    "For 'parity' (n-XOR), if number of attributes is odd, output is false, otherwise it's true; size of tree is exponential, hard problem\n",
    "\n",
    "For booleans, 2^n rows\n",
    "Possible output combinations is 2^(2^n) (huge number!)\n",
    "\n",
    "### ID3\n",
    "\n",
    "Loop:\n",
    "1. A <- best attribute (information gain most popular way to determine best attribute)\n",
    "2. Assign A as decision attribute for noe\n",
    "3. For each value of A, create a descendant of node\n",
    "4. Sort training examples to leaves\n",
    "5. If examples perfectly classified, stop; else iterate over leaves\n",
    "\n",
    "Information gain looks for greatest reduction in randomness<br>\n",
    "Gain(S, A) = Entropy(S) - average entropy over each set of examples with a particular value<br>\n",
    "(S = collection of training examples, A = particular attribute)<br>\n",
    "\n",
    "Entropy = sum of all possible values (v) times probability of v, times log of probability of v times -1\n",
    "\n",
    "### ID3 Bias\n",
    "\n",
    "Restriction bias: hypothesis that you actually care about it (e.g. decided to use DecisionTree as the model, don't care about other models)\n",
    "Preference bias (inductive bias): what sort of hypothesis we prefer, i.e. what sort of decision tree we prefer\n",
    "\n",
    "Types of trees we prefer (with ID3):\n",
    "* good splits at top\n",
    "* correct over incorrect\n",
    "* shorter trees\n",
    "\n",
    "### Decision Trees Continuous Attributes\n",
    "\n",
    "How to deal with continuous attributes?\n",
    "\n",
    ">Separate into bins (i.e. for age, bins with 0-10, 11-20, etc.), then convert into T/F questions (i.e. 20 <= AGE <= 30, T/F)\n",
    "\n",
    "### Other Considerations\n",
    "\n",
    "Does it make sense to repeat an attribute along a path in the tree?\n",
    "\n",
    ">Yes, if you're asking a different question about the same attribute.\n",
    "\n",
    "When do we stop?\n",
    "\n",
    "* When everything's classified correctly\n",
    "* No more attributes\n",
    "* No overfitting\n",
    "\n",
    "Cross validation, check data against validation set\n",
    "More efficient: check against validation set each time you expand the tree, if the error's low enough, stop expanding the tree\n",
    "Pruning--collapse leaves back up into tree, how does that affect the error on my validation data? If the error's small, go ahead and do the pruning\n",
    "\n",
    "How to adapt decision trees for regression?\n",
    "* Splitting criteria?\n",
    "* Variance?\n",
    "* output: average (type of voting), local linear fit?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
