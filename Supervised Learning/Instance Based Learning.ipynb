{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Based Learning Before/Now\n",
    "\n",
    "take all the training data, put it in a database, then look it up to make predictions on new data\n",
    "\n",
    "remembers things\n",
    "very fast\n",
    "very simple\n",
    "\n",
    "doesn't generalize, very sensitive to noise\n",
    "\n",
    "\n",
    "### K Nearest Neighbors\n",
    "\n",
    "Given:\n",
    "\n",
    ">Training Data D = {x$_i$, y$_i$}<br>\n",
    "Distance Metric d(q, x) (domain knowledge)<br>\n",
    "Number of Neighbors K (domain knowledge)<br>\n",
    "Query Point q\n",
    "\n",
    "NN = {i: d(q, x$_i$), K smallest}<br>\n",
    "(all the elements in data closest (K closest) to the data point (by determined distance metric)\n",
    "\n",
    "Return:\n",
    "\n",
    ">classification: vote of the y$_i$'s that are nearest (NN) (plurality)<br>\n",
    "regression: mean of the y$_i$'s of NN (weighted by distance)\n",
    "\n",
    "Cheap to learn, expensive to query (only logarithmic, but you potentially may need to query many times, while training generally only happens once)\n",
    "\n",
    "Considered 'lazy learner' vs. 'eager learner', such as linear regression and most other machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Domain K NNowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1, distance='euclidean':  8.0\n",
      "K=3, distance='euclidean':  42.0\n",
      "K=1, distance='manhattan':  29.0\n",
      "K=3, distance='manhattan':  35.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from operator import itemgetter\n",
    "A = [4,2]\n",
    "\n",
    "X = [[1,6], [2,4], [3,7], [6,8], [7,1], [8,4]]\n",
    "\n",
    "def k_nearest_mean(A, X, k, dist_function='euclidean'):\n",
    "    X_points = []\n",
    "    for v in X:\n",
    "        if dist_function in ['minkowski', 'manhattan']:\n",
    "            dist = distance.minkowski(A, v, 1)\n",
    "        else:\n",
    "            dist = distance.euclidean(A, v)\n",
    "        X_points.append({'x': v, 'd': dist, 'y': v[0]**2 + v[1]})\n",
    "    X_points.sort(key=itemgetter('d'))\n",
    "    k_points = X_points[:k]\n",
    "    for point in X_points[k:]:\n",
    "        if point['d'] == k_points[-1]['d']:\n",
    "            k_points.append(point)\n",
    "        else:\n",
    "            break\n",
    "    return np.mean([point['y'] for point in k_points])      \n",
    "\n",
    "print(\"K=1, distance='euclidean': \", k_nearest_mean(A, X, 1))\n",
    "print(\"K=3, distance='euclidean': \", k_nearest_mean(A, X, 3))\n",
    "print(\"K=1, distance='manhattan': \", k_nearest_mean(A, X, 1, 'minkowski'))\n",
    "print(\"K=3, distance='manhattan': \", k_nearest_mean(A, X, 3, 'minkowski'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN Bias\n",
    "\n",
    "Preference bias:\n",
    "\n",
    "* Locality: near point are similar\n",
    "* Smoothness: averaging\n",
    "* All features matter equally\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially (curse applies to ML in general, not just KNN)\n",
    "\n",
    "Better off giving the model more data than giving it more dimensions\n",
    "\n",
    "### Some Other Stuff\n",
    "\n",
    "* distance metric; choice of distance metric/function has a huge impact; euclidean & manhattan really useful for regression\n",
    "* weighting makes a difference on distance, can also help with dimensionality problem\n",
    "* K = n (taking all data points and averaging y values together; basically ignoring the query)\n",
    "* But what if you do a weighted average? Points near query weighted more heavily than the average, so it does matter where you put your query point; locally-weighted regression; in place of averaging function, you can use a decision tree, neural network, linear regression, pretty much anything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
