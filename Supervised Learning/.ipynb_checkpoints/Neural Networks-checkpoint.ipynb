{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Work like nerve cells in the human brain; fire when certain conditions are present\n",
    "Artificial Neural Networks (ANN): cartoonish version of NN\n",
    "\n",
    "Perceptron:\n",
    "inputs multiplied by weights, then summed, to make the activation value\n",
    "if activation >= firing threshold, output = 1\n",
    "else output = 0\n",
    "\n",
    "### How Powerful Is a Perceptron Unit\n",
    "\n",
    "Perceptrons always compute halfplanes (above 1, below 0)\n",
    "\n",
    "Can perform boolean operations (AND, OR, NOT, XOR, etc.)\n",
    "\n",
    "### Perceptron Training\n",
    "\n",
    "Given examples, find weights that map inputs to output\n",
    "\n",
    "* perceptron rule (threshold)\n",
    "* gradient descent/delta rule (unthresholded)\n",
    "\n",
    "#### Perceptron rule\n",
    "\n",
    "Set the weights so that you capture the training/testing data\n",
    "Set learning rule for the weights, but not for theta (subtract theta from both sides)\n",
    "Added 'bias' as 1, with weight as -1 (subtracted theta from both sides)\n",
    "Iterate over training data, grab X (features) and y (target), and change weight for each feature\n",
    "Weight change--take target, compare it to sum of features that is greater than or equal to zero\n",
    "If output is correct (difference is zero), no change\n",
    "If output is wrong, decrease or increase weights at the scale of the learning rate until output is zero\n",
    "\n",
    "y = target<br>\n",
    "$\\hat{y}$ = output<br>\n",
    "n = learning rate<br>\n",
    "x = input<br>\n",
    "\n",
    "w$_i$ = w$_i$ + $\\Delta$w # weight equals weight plus the change in the weight (just means we're changing the weight as we iterate<br>\n",
    "$\\Delta$w = n(y - $\\hat{y}$)x$_i$ # change in weight is equal to the learning rate times (target - output) times input<br>\n",
    "$\\hat{y}$ = ($\\Sigma_i$w$_i$x$_i$ >= 0) # output = sum of all input weights times the features is greater than or equal to 0<br>\n",
    "\n",
    "If dataset is linearly separable, it will iterate until it finds it<br>\n",
    "(put condition in it to stop the loop if there's no more error)<br>\n",
    "If it's not linearly separable...\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "More robust to non-linearly separable data\n",
    "Imagine the output is not thresholded\n",
    "Define error metric on weight vector, use calculus to push squared error downward toward zero\n",
    "\n",
    "$\\Delta$w = n(y - a)x$_i$, where a = activation (same as perceptron rule but w/out threshold)\n",
    "\n",
    "Can't do gradient descent on $\\hat{y}$ because it's non-differentiable\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "Makes perceptron rule differentiable (analagous to perceptron, not equal to it)\n",
    "\n",
    "(S-like function)\n",
    "\n",
    "As activation gets smaller, function goes toward 0\n",
    "As activation gets bigger, function goes toward 1\n",
    "\n",
    "### Neural Network Sketch\n",
    "\n",
    "hidden layer of sigmoid units (or other differentiable functions), whole thing is differentiable\n",
    "can calculate how to move the weights in order to get closer to the output you want\n",
    "back propagation = computationally beneficial organization of the chain rule\n",
    "\n",
    "many local optima--can get 'stuck' in spot where you can't change any individual weight without making the error worse\n",
    "plots where there are several 'local' valleys, where it goes down to a local low point but then goes back up, not the global optimum\n",
    "\n",
    "### Optimizing Weights\n",
    "\n",
    "gradient descent isn't the only method\n",
    "advanced methods:\n",
    "* momentum (momentum turns in the gradient, to keep from getting stuck in a local minimum)\n",
    "* higher-order derivatives (look at combinations of weights instead of individual weights)\n",
    "* randomized optimization\n",
    "* penalty for \"complexity\" (overfitting, more nodes, more layers, large numbers)\n",
    "\n",
    "### Restriction Bias\n",
    "\n",
    "* Representational power\n",
    "* set of hypotheses we will consider\n",
    "\n",
    "perceptron: half spaces\n",
    "sigmoids: much more complex, not much restriction\n",
    "\n",
    "Boolean: network of threshold-like units\n",
    "Continuous: \"connected\", no jumps - hidden\n",
    "Arbitrary: stitch together - two hidden\n",
    "\n",
    "can represent pretty much anything with a sufficiently complex network\n",
    "danger of overfitting!\n",
    "usually bounded number of hidden units and layers, to reduce complexity (and more restriction bias)\n",
    "can use cross validation to check overfitting\n",
    "\n",
    "### Preference Bias\n",
    "\n",
    "* algorithm's selection of one representation over another\n",
    "What algorithm?\n",
    "How to initialize weights?\n",
    "typically use small random values\n",
    "* adds variability, helpful to avoid local minima\n",
    "* low complexity\n",
    "prefer correct over incorrect\n",
    "prefer simpler answers to more complex ones\n",
    "\n",
    "Occam's razor: entities should not be multiplied unnecessarily\n",
    "\n",
    "multiply = make more complex\n",
    "unnecessary = not reducing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input layer\n",
    "input_layer = np.array([1, 2, 3])\n",
    "\n",
    "# two hidden layers (the weights)\n",
    "hidden_layer_1 = np.array([1, 1, -5])\n",
    "hidden_layer_2 = np.array([3, -4, 2])\n",
    "\n",
    "# multiply the input by each hidden layer\n",
    "y1 = np.dot(input_layer, hidden_layer_1)\n",
    "y2 = np.dot(input_layer, hidden_layer_2)\n",
    "y = np.array([y1, y2])\n",
    "\n",
    "# then multiply that by the output layer\n",
    "z = np.dot(y, [2,-1])\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 1]\n",
    "\n",
    "w1 = [3, 2]\n",
    "w2 = [-1, 4]\n",
    "w3 = [3, -5]\n",
    "\n",
    "y1 = np.dot(x, w1)\n",
    "y2 = np.dot(x, w2)\n",
    "y3 = np.dot(x, w3)\n",
    "\n",
    "z = np.dot([y1, y2, y3], [1, 2, -1])\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
