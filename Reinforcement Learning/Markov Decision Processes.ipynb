{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Making & Reinforcement Learning\n",
    "\n",
    "* Supervised learning = function approximation\n",
    "* Unsupervised learning = clustering description\n",
    "* Reinforcement learning = looks a lot like function approximation, but has a very different character; one mechanism for doing decision making\n",
    "\n",
    "### Markov Decision Processes (MDPs)\n",
    "(Using grid example to illustrate)\n",
    "\n",
    "* States\n",
    "    * Set of tokens that represent every state that one could be in (which grid position I'm in)\n",
    "* Model (or transition model)\n",
    "    * Rules of the game\n",
    "    * Probability that you will end up transitioning to a given state, given a state and an action\n",
    "    * (Probability that you will end up in the state immediately above you, if you take the action 'Up', given your current state)\n",
    "* Actions\n",
    "    * Things you can do in a particular state (four directions you can go, depending on current grid position)\n",
    "* Reward\n",
    "    * Scalar value that you receive for being in a state\n",
    "    * Tells you the usefulness for entering into that state, or being in a state and taking action, or being in a state, taking an action, and entering into another state\n",
    "* Policy\n",
    "    * The solution; takes in a state and returns an action, a command\n",
    "    * The optimal policy is the one that maximizes your long-term expected reward\n",
    "    \n",
    "training data is usually (s, a, r), i.e. for each, record, the state, action, and reward\n",
    "    \n",
    "Markovian properties\n",
    "* only the present matters\n",
    "    * means that the transition model only depends on where you are now (current state), not where you were n number of steps ago\n",
    "* world is stationary\n",
    "    * rules don't change\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3276800000000001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8 ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3277600000000001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8 ** 5) + ((0.1 ** 4) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
