{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Making & Reinforcement Learning\n",
    "\n",
    "* Supervised learning = function approximation\n",
    "* Unsupervised learning = clustering description\n",
    "* Reinforcement learning = looks a lot like function approximation, but has a very different character; one mechanism for doing decision making\n",
    "\n",
    "### Markov Decision Processes (MDPs)\n",
    "(Using grid example to illustrate)\n",
    "\n",
    "* States\n",
    "    * Set of tokens that represent every state that one could be in (which grid position I'm in)\n",
    "* Model (or transition model)\n",
    "    * Rules of the game\n",
    "    * Probability that you will end up transitioning to a given state, given a state and an action\n",
    "    * (Probability that you will end up in the state immediately above you, if you take the action 'Up', given your current state)\n",
    "* Actions\n",
    "    * Things you can do in a particular state (four directions you can go, depending on current grid position)\n",
    "* Reward\n",
    "    * Scalar value that you receive for being in a state\n",
    "    * Tells you the usefulness for entering into that state, or being in a state and taking action, or being in a state, taking an action, and entering into another state\n",
    "    * Delayed reward (actions set you up for actions, then for other actions, that eventually lead to rewards)\n",
    "        * Which action led to ultimately getting the reward? (temporal credit assignment problem)\n",
    "    * Minor changes to reward function matter\n",
    "    * Having a small negative reward everywhere encourages you to end the game as soon as possible\n",
    "    * Rewards are the domain knowledge\n",
    "* Policy\n",
    "    * The solution; takes in a state and returns an action, a command\n",
    "    * The optimal policy is the one that maximizes your long-term expected reward\n",
    "    \n",
    "training data is usually (s, a, r), i.e. for each, record, the state, action, and reward\n",
    "    \n",
    "Markovian properties\n",
    "* only the present matters\n",
    "    * means that the transition model only depends on where you are now (current state), not where you were n number of steps ago\n",
    "* world is stationary\n",
    "    * rules don't change\n",
    "    \n",
    "### Sequences of Rewards: Assumptions\n",
    "\n",
    "* Infinite horizons (infinite number of steps to get to goal)\n",
    "    * Means infinite amount of time to get to the reward, action not affected by time\n",
    "* Finite horizons\n",
    "    * Have to take into acount time\n",
    "* Utility of sequences (stationarity of preferences)\n",
    "    * If I prefer one sequence of states today over another sequence, then I prefer the same sequence tomorrow (history of steps doesn't matter)\n",
    "* With all positive rewards, infinite horizon, sum of rewards will be infinity\n",
    "* Discounted rewards or sums, allows us to go infinite distance in finite time, bounds the reward to some value (gamma)\n",
    "    * Turns infinite horizons into finite steps\n",
    "    * Works by summing a value t times (rather than summing R) between 0 and 1 (gamma), and then multiplying it by the greatest value for R (reward); ensures total value will never be larger than the greates value for R (Rmax)\n",
    "    \n",
    "### Policies: Finding Policies (Value Iteration)\n",
    "* Start with arbitrary utilities\n",
    "* Update utilities based on neighbors (based on all states they can reach) at every iteration\n",
    "    * With each step, adding truth (actual reward), get closer and closer to the true utilities of the states\n",
    "    * Can find optimal policy from the true utilities\n",
    "* Repeat until convergence\n",
    "\n",
    "* Start with an initial policy (guess)\n",
    "* Evaluate\n",
    "    * Given a policy, calculate utility of that policy\n",
    "* Improve\n",
    "    * Update policy by taking the actions that maximize the expected utility"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
