{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement = reward maximization\n",
    "\n",
    "model-based reinforcement learner = takes transitions, generates policy\n",
    "reinforcement-based planner = takes model, generates transitions from simulator, fits to learner, generates policy\n",
    "\n",
    "### Three Approaches to RL\n",
    "\n",
    "* Policy search\n",
    "    * Map states to actions, via policy\n",
    "    * Direct use, indirect learning\n",
    "* Value-function based\n",
    "    * Map states to values, via utility\n",
    "* Model based\n",
    "    * Direct learning, indirect use\n",
    "    \n",
    "### Value-Function Based Approach\n",
    "\n",
    "\n",
    "\n",
    "### Q Learning\n",
    "\n",
    "Value of arriving in some state S = Reward for S + discount expected value for taking action A + action with the highest Q value\n",
    "i.e. value for arriving in S, leaving via A, proceeding optimally thereafter\n",
    "\n",
    "U of S returns a scalar\n",
    "Pi of S returns an action\n",
    "\n",
    "Turn Q into U if you always choose the best action\n",
    "\n",
    "U sub S = Max over A of Q sub S comma A\n",
    "Pi sub S = argmax over A of Q sub S comma A\n",
    "\n",
    "How to find Q? (essence of Q learning)\n",
    "\n",
    "Evaluating the Bellman equations from data\n",
    "\n",
    "#### Estimating Q from transitions\n",
    "\n",
    "In learning scenario, we don't have T and R, we have transitions\n",
    "\n",
    "[s, a, r, s$'$] (state, action, reward, s prime (new state))\n",
    "\n",
    "Learn incrementally\n",
    "\n",
    "V of t learns at a rate of alpha of t toward X of t\n",
    "weights (alpha) decay over time, converges toward the mean\n",
    "\n",
    "Q-learning is a family of algorithms, diverging as follows:\n",
    "\n",
    "* How do we initialize Q hat?\n",
    "* How do we decay alpha sub t?\n",
    "* How do we choose actions?\n",
    "    * Use Q-hat\n",
    "    * Only time this is bad is if each action results in Q worse than just choosing a sub zero every time, then it will always choose a sub zero every time (known as greedy exploration) \"local minimum\"\n",
    "        * To solve this--random restarts\n",
    "            * Take uphill steps, but occasionally (randomly) take a downhill step, more like a random action every once in a while\n",
    "            * Lets you explore the whole space, so you can find the optimal Q\n",
    "        * You can also achieve this by trying different starting points for Q\n",
    "            \n",
    "#### Epsilon-Greedy Exploration\n",
    "\n",
    "GLIE (\"greedy limit + infinite exploration\")\n",
    "\n",
    "Start off more random, over time get less and less random and more greedy\n",
    "Policy that we're following becomes more and more like the optimal policy over time\n",
    "\n",
    "this is a solution for the Exploration-Exploitation dilemma (fundamental trade-off in reinforcement learning)\n",
    "* Exploration: getting the data you need so you can learn\n",
    "* Exploitation: using what you know\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
