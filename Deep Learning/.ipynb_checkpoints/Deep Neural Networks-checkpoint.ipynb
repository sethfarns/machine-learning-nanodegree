{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "Perceptron checks inputs times their weights, plus the bias, and checks if it's positive or negative\n",
    "\n",
    "Wx + b >= 0\n",
    "\n",
    "## Why 'Neural Networks'?\n",
    "\n",
    "Neurons in the brain take inputs, perform some calculations on them, and decide to 'fire' or not; similarly, in with deep neural networks, we take weighted inputs, perform calculations, and decide to output a 1 or a 0\n",
    "\n",
    "## Perceptrons as Logical Operators\n",
    "\n",
    "#### AND Perceptron\n",
    "\n",
    "If all inputs are 'true', output is 'true', otherwise output is 'false'\n",
    "\n",
    "#### OR perceptron\n",
    "\n",
    "If any inputs are 'true', output is 'true', otherwise output is 'false'\n",
    "\n",
    "#### XOR perceptron\n",
    "\n",
    "If exactly one input is 1 and one is 0, output 'true', otherwise 'false'\n",
    "\n",
    "## XOR Multi-Layer Perceptron\n",
    "\n",
    "NAND (AND + NOT) and OR, then AND from the output of both to get XOR\n",
    "\n",
    "## Perceptron Trick\n",
    "\n",
    "For moving the line to better classify incorrectly classified points\n",
    "\n",
    "Take the equation for the separating line, subtract (for negative point classified as positive, or add for positive point classified as negative) the incorrectly classified point multiplied by a determined factor (the learning rate), to get the equation for the new separating line\n",
    "\n",
    "## Error Function\n",
    "\n",
    "To use gradient descent, the error function must be continuous and differentiable. In order to do this, we have to do continuous predictions instead of discrete. Just change the step function (0 or 1) to the sigmoid function.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "## Softmax\n",
    "\n",
    "Equivalent of sigmoid activation function, but when the problem has 3 or more classes\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "Maximize probability of all points. Use the natural log of the probabilities rather than the product\n",
    "\n",
    "## Cross Entropy\n",
    "See function below\n",
    "\n",
    "## Multi-class Cross Entropy\n",
    "\n",
    "more complicated...\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Error function\n",
    "\n",
    "If y == 1<br>\n",
    "P(pos) = $\\hat{y}$<br>\n",
    "Error = -ln($\\hat{y}$)\n",
    "\n",
    "If y == 0<br>\n",
    "P(neg) = 1 - P(pos) = 1 - $\\hat{y}$<br>\n",
    "Error = -ln(1 - $\\hat{y}$)\n",
    "\n",
    "Summarized into one function:\n",
    "$$Error = -(1-y)(ln(1-\\hat{y})) - yln(\\hat{y})$$\n",
    "\n",
    "$$ Error function = -\\frac{1}{m}\\sum_{i=1}^{m} (1-y_i)(ln(1-\\hat{y_i})) + y_{i}ln(\\hat{y_i}) $$\n",
    "\n",
    "$$ Multiclass error function = -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}y_{ij}ln(\\hat{y}_{ij}) $$\n",
    "\n",
    "$$ E(W,b) = -\\frac{1}{m}\\sum_{i=1}^{m} (1-y_i)(ln(1-\\sigma(Wx^{(i)}+b))) + y_{i}ln(\\sigma(Wx^{(i)}+b)) $$\n",
    "\n",
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "Gradient descent: Change $w_i$ to $$ w_{i} + a(y-\\hat{y})x_{i} $$\n",
    "\n",
    "Perceptron algorithm: If x is misclassified, change $w_i$ to $w_i$ + a x$_i$ if positive, or\n",
    "$$ w_{i} - a x_{i} $$ if negative\n",
    "\n",
    "\n",
    "## Non-linear Models\n",
    "\n",
    "* Combine multiple linear models into a non-linear model\n",
    "* Take probability of each point being positive or negative for each model\n",
    "* Combine these probabilities by adding them together and then applying the sigmoid function\n",
    "* Can add weights to each probability to make one model more important than the others\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "* Create deep learning network by combining linear models to create non-linear models, then combining those to create more non-linear models\n",
    "    * This is where the magic of deep learning happens!\n",
    "* Input layer: weights\n",
    "* Hidden layer: where linear models are combined\n",
    "* Output layer: final non-linear model\n",
    "* One output node for each class (for multi-class classification), with each output being the probability that the point lies in that class\n",
    "\n",
    "## Feedforward\n",
    "\n",
    "* Process of multiplying weights by matrices (for each layer) with the sigmoid function until we get the final prediction\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "* Do feedforward operation\n",
    "* Compare out of model with desired output\n",
    "* Calculate error\n",
    "* Run the feedforward operation backwards (backpropagation) to spread the error to each of the weights\n",
    "* Use this to update the weights, and get a better model\n",
    "* Continue this until we have a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perceptron_combiner(w1, w1_weight, w2, w2_weight, b):\n",
    "    return sigmoid_function(w1 * w1_weight + w2 * w2_weight + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(perceptron_combiner(2, 0.4, 6, 0.6, -2))\n",
    "print(perceptron_combiner(3, 0.4, 5, 0.6, -2.2))\n",
    "print(perceptron_combiner(5, 0.4, 4, 0.6, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-1 * math.log(0.7 * 0.9 * 0.8 * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_cross_entropy([1, 1, 0], [0.8, 0.7, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log\n",
    "\n",
    "# minimize the cross entropy to improve model   \n",
    "def binary_cross_entropy(labels, probabilities):\n",
    "    return -1 * sum([(y*log(p)) + ((1-y)*log(1-p)) for p,y in zip(probabilities, labels)])\n",
    "\n",
    "def softmax(L):\n",
    "    total = sum([np.exp(x) for x in L])\n",
    "    return [np.exp(x) / total for x in L]\n",
    "\n",
    "def perceptron_trick(current_line, point, learning_rate=0.1, current_positive=True):\n",
    "    if current_positive:\n",
    "        new_line = [round(x - (learning_rate*y), 2) for x,y in zip(current_line, point + [1])]\n",
    "    else:\n",
    "        new_line = [round(x + (learning_rate*y), 2) for x,y in zip(current_line, point + [1])]\n",
    "    return new_line\n",
    "\n",
    "def sigmoid_function(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def step_function(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron_classify(equation, point, function='sigmoid'):\n",
    "    computed = [x * y for x, y in zip(equation, point + [1])]\n",
    "    if function == 'sigmoid':\n",
    "        return sigmoid_function(sum(computed))\n",
    "    else:\n",
    "        return step_function(sum(computed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron_classify([-4, 5, -9], [4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron_classify([4, 5, -9], [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = [3, 4, -10]\n",
    "point = [1, 1]\n",
    "lr = 0.1\n",
    "cp = False\n",
    "counter = 0\n",
    "while perceptron_classify(line, point) == 0:\n",
    "    line = perceptron_trick(line, point, lr, cp)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "41.5 + 38.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def and_perceptron(x1, x2):\n",
    "    weight = 1\n",
    "    bias = -1.5\n",
    "    weighted_inputs = [weight * x for x in [x1, x2]]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "and_perceptron(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def or_perceptron(x1, x2):\n",
    "    weight = 1\n",
    "    bias = -0.5\n",
    "    weighted_inputs = [weight * x for x in [x1, x2]]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "or_perceptron(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def not_perceptron(x1, x2):\n",
    "    w1 = 0.0\n",
    "    w2 = -1.0\n",
    "    bias = 0.5\n",
    "    weighted_inputs = [w * x for w, x in zip([w1, w2], [x1, x2])]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_perceptron(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xor_perceptron(x1, x2):\n",
    "    if or_perceptron(x1, x2) == 1 and and_perceptron(x1, x2) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xor_perceptron(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keras.models.Sequential` class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like `compile()`, `fit()`, and `evaluate()` that are used to train and run the model. We'll cover these functions soon, but first let's start looking at the layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras requires the input shape to be specified in the first layer, but it will automatically infer the shape of all other layers. This means you only have to explicitly set the input dimensions for the first layer.\n",
    "\n",
    "The first (hidden) layer from above, `model.add(Dense(32, input_dim=X.shape[1]))`, creates 32 nodes which each expect to receive 2-element vectors as inputs. Each layer takes the outputs from the previous layer as inputs and pipes through to the next layer. This chain of passing output to the next layer continues until the last layer, which is the output of the model. We can see that the output has dimension 1.\n",
    "\n",
    "The activation \"layers\" in Keras are equivalent to specifying an activation function in the Dense layers (e.g., `model.add(Dense(128))`; `model.add(Activation('softmax'))` is computationally equivalent to `model.add(Dense(128, activation=\"softmax\"))`), but it is common to explicitly separate the activation layers because it allows direct access to the outputs of each layer before the activation is applied (which is useful in some model architectures).\n",
    "\n",
    "Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data. We'll specify the loss function to be `categorical_crossentropy` which can be used when there are only two classes, and specify `adam` as the optimizer (which is a reasonable default when speed is a priority). And finally, we can specify what metrics we want to evaluate the model with. Here we'll use accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained with the `fit()` method, through the following command that specifies the number of training epochs and the message level (how much information we want displayed on the screen during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 9         \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Predictions:\n",
      "[[ 0.42938694]\n",
      " [ 0.55750358]\n",
      " [ 0.53029174]\n",
      " [ 0.45875016]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Using TensorFlow 1.0.0; use tf.python_io in later versions\n",
    "tf.python_io.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "\n",
    "# Add required layers\n",
    "xor.add(Dense(8, input_dim=2))\n",
    "xor.add(Activation('tanh'))\n",
    "xor.add(Dense(1))\n",
    "xor.add(Activation('sigmoid'))\n",
    "\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "xor.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, epochs=250, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Optimization\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
