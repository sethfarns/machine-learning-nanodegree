{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of CNNs\n",
    "\n",
    "RNNs used more frequently in NLP, though CNNs can be used in this area too\n",
    "\n",
    "* [Text classification](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n",
    "* [Language translation](https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/)\n",
    "* Play [Atari games](https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/) with a CNN and reinforcement learning\n",
    "* Power drones\n",
    "* Self-driving cars\n",
    "* [Localize breast cancer](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html)\n",
    "\n",
    "## How Computers Interpret Images\n",
    "\n",
    "* MNIST database, 70,000 images of hand-written digits (available in `keras.datasets`)\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "## Issues with MLPs\n",
    "* Only use fully connected/densely connected layers (each node in one layer connects with a certain weight to every node in the following layer)\n",
    "* Only accept vectors as input\n",
    "\n",
    "## How CNNs address these issues\n",
    "* Use sparsely connected/locally connected layers\n",
    "    * Hidden nodes connected to pixels only in a certain region (i.e. an image matrix can be broken into four regions, with each of four hidden nodes connected to pixels from one region)\n",
    "* Accepts matrics as input\n",
    "* Weight-sharing (if a cat is seen in one portion of an image, those weights are shared for other potions of the image)\n",
    "\n",
    "## Convolutional Layers\n",
    "\n",
    "* Slide a convolution window over every equally-sized region in the matrix; at each position, the window specifies a small piece of the matrix, and connects each collection of pixels to a single hidden layer, called a convolutional layer\n",
    "\n",
    "## Stride and Padding\n",
    "\n",
    "Stride: amount by which the filter strides across the image\n",
    "Stride of 1 makes the convolutional layer roughly the height and width of the input parameter; if stride is 2, the height and width are about half of the input layer\n",
    "Padding: pad layer with zeros, to make sure we don't lose some nodes from the convolutional layer ('same'); otherwise, you lose some data on the edges ('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convolutional Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "You must pass the following arguments:\n",
    "\n",
    "* `filters` - The number of filters.\n",
    "* `kernel_size` - Number specifying both the height and width of the (square) convolution window.\n",
    "\n",
    "There are some additional, optional arguments that you might like to tune:\n",
    "\n",
    "* `strides` - The stride of the convolution. If you don't specify anything, strides is set to 1.\n",
    "* `padding` - One of 'valid' or 'same'. If you don't specify anything, padding is set to 'valid'.\n",
    "* `activation` - Typically 'relu'. If you don't specify anything, no activation is applied. You are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks.\n",
    "\n",
    "NOTE: It is possible to represent both kernel_size and strides as either a number or a tuple.\n",
    "\n",
    "When using your convolutional layer as the first layer (appearing after the input layer) in a model, you must provide an additional input_shape argument:\n",
    "\n",
    "* `input_shape` - Tuple specifying the height, width, and depth (in that order) of the input.\n",
    "\n",
    "NOTE: Do not include the input_shape argument if the convolutional layer is not the first layer in your network.\n",
    "\n",
    "There are many other tunable arguments that you can set to change the behavior of your convolutional layers. To read more about these, we recommend perusing the official documentation.\n",
    "\n",
    "#### Example no. 1\n",
    "\n",
    "Say I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1). Then, say I'd like the next layer to be a convolutional layer with 16 filters, each with a width and height of 2. When performing the convolution, I'd like the filter to jump two pixels at a time. I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros. Then, to construct this convolutional layer, I would use the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7f97925b6470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example no. 2\n",
    "\n",
    "Say I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 filters, each with a height and width of 3. When performing the convolution, I'd like the filter to jump 1 pixel at a time. I want the convolutional layer to see all regions of the previous layer, and so I don't mind if the filter hangs over the edge of the previous layer when it's performing the convolution. Then, to construct this convolutional layer, I would use the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7f97925b6438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example no. 3\n",
    "\n",
    "If you look up code online, it is also common to see convolutional layers in Keras in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7f97925b6400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(64, (2,2), activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there are 64 filters, each with a size of 2x2, and the layer has a ReLU activation function. The other arguments in the layer use the default values, so the convolution uses a stride of 1, and the padding has been set to 'valid'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 16)      80        \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='valid', \n",
    "    activation='relu', input_shape=(200, 200, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula: Number of Parameters in a Convolutional Layer\n",
    "\n",
    "The number of parameters in a convolutional layer depends on the supplied values of filters, kernel_size, and input_shape. Let's define a few variables:\n",
    "\n",
    "* K - the number of filters in the convolutional layer\n",
    "* F - the height and width of the convolutional filters\n",
    "* D_in - the depth of the previous layer\n",
    "\n",
    "Notice that K = filters, and F = kernel_size. Likewise, D_in is the last value in the input_shape tuple.\n",
    "\n",
    "Since there are F*F*D_in weights per filter, and the convolutional layer is composed of K filters, the total number of weights in the convolutional layer is K*F*F*D_in. Since there is one bias term per filter, the convolutional layer has K biases. Thus, the number of parameters in the convolutional layer is given by K*F*F*D_in + K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula: Shape of a Convolutional Layer\n",
    "\n",
    "The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. Let's define a few variables:\n",
    "\n",
    "* K - the number of filters in the convolutional layer\n",
    "* F - the height and width of the convolutional filters\n",
    "* S - the stride of the convolution\n",
    "* H_in - the height of the previous layer\n",
    "* W_in - the width of the previous layer\n",
    "\n",
    "Notice that K = filters, F = kernel_size, and S = stride. Likewise, H_in and W_in are the first and second value of the input_shape tuple, respectively.\n",
    "\n",
    "The depth of the convolutional layer will always equal the number of filters K.\n",
    "\n",
    "If padding = 'same', then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "* height = ceil(float(H_in) / float(S))\n",
    "* width = ceil(float(W_in) / float(S))\n",
    "\n",
    "If padding = 'valid', then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "* height = ceil(float(H_in - F + 1) / float(S))\n",
    "* width = ceil(float(W_in - F + 1) / float(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "See [Keras documentation](https://keras.io/layers/pooling/)\n",
    "\n",
    "Method for reducing dimensionality within CNN\n",
    "\n",
    "1. Max pooling layer\n",
    "    * Takes stack of feature maps as input\n",
    "    * Specify window size and stride\n",
    "    * Take the maximum of the pixels in each window to construct the max pooling layer\n",
    "2. Global average pooling\n",
    "    * More extreme form of dimensionality reduction\n",
    "    * Takes stack of feature maps, calculates average value for nodes in each\n",
    "    * Final output is stack of feature maps where each feature map has been reduced to a single value\n",
    "    * Turns 3D array into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling Layers in Keras\n",
    "\n",
    "To create a max pooling layer in Keras, you must first import the necessary module:\n",
    "\n",
    "```python\n",
    "from keras.layers import MaxPooling2D\n",
    "```\n",
    "\n",
    "Then, you can create a convolutional layer by using the following format:\n",
    "\n",
    "```python\n",
    "MaxPooling2D(pool_size, strides, padding)\n",
    "```\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "You must include the following argument:\n",
    "\n",
    "* pool_size - Number specifying the height and width of the pooling window.\n",
    "\n",
    "There are some additional, optional arguments that you might like to tune:\n",
    "\n",
    "* strides - The vertical and horizontal stride. If you don't specify anything, strides will default to pool_size.\n",
    "* padding - One of 'valid' or 'same'. If you don't specify anything, padding is set to 'valid'.\n",
    "\n",
    "NOTE: It is possible to represent both pool_size and strides as either a number or a tuple.\n",
    "\n",
    "You are also encouraged to read the official documentation.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Say I'm constructing a CNN, and I'd like to reduce the dimensionality of a convolutional layer by following it with a max pooling layer. Say the convolutional layer has size (100, 100, 15), and I'd like the max pooling layer to have size (50, 50, 15). I can do this by using a 2x2 window in my max pooling layer, with a stride of 2, which could be constructed in the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.MaxPooling2D at 0x7f97925b6f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "MaxPooling2D(pool_size=2, strides=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Dimensionality of Max Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs for Image Classification\n",
    "\n",
    "* CNNs require fixed-size images\n",
    "    * Generally, resize them all to a square\n",
    "    * Every image interpreted as 3D array\n",
    "        * Grayscale images have depth of 1, still technically 3D\n",
    "* CNNs make layers deeper, pooling layers decrease dimensionality\n",
    "* Set kernel size to 2, stride to 1, padding to 'same'\n",
    "    * Gives CNN layer same width and height as previous layer\n",
    "* Number of filters often doubles with each layer (16, 32, 64, etc.)\n",
    "* Use 'relu' activation function in all CNN layers\n",
    "* Max pooling layers generally follow every one or two convolutional layers in the sequence\n",
    "    * Generally half the dimensions (pool_size, stride equal 2)\n",
    "* Resultant layer contains no spatial image (spatial information no longer important)\n",
    "* Flatten array to vector, feed to one or more fully connected layers to determine what object was found in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network begins with a sequence of three convolutional layers, followed by max pooling layers. These first six layers are designed to take the input array of image pixels and convert it to an array where all of the spatial information has been squeezed out, and only information encoding the content of the image remains. The array is then flattened to a vector in the seventh layer of the CNN. It is followed by two dense layers designed to further elucidate the content of the image. The final layer has one entry for each object class in the dataset, and has a softmax activation function, so that it returns probabilities.\n",
    "\n",
    "#### Things to Remember\n",
    "\n",
    "* Always add a ReLU activation function to the Conv2D layers in your CNN. With the exception of the final layer in the network, Dense layers should also have a ReLU activation function.\n",
    "* When constructing a network for classification, the final layer in the network should be a Dense layer with a softmax activation function. The number of nodes in the final layer should equal the total number of classes in the dataset.\n",
    "* Have fun! If you start to feel discouraged, we recommend that you check out [Andrej Karpathy's tumblr](https://lossfunctions.tumblr.com/) with user-submitted loss functions, corresponding to models that gave their owners some trouble. Recall that the loss is supposed to decrease during training. These plots show very different behavior :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation in Keras\n",
    "\n",
    "We're only interested in whether an object is present in the image, therefore:\n",
    "* Scale invariance: we don't want the size to matter\n",
    "* Rotation invariance: we don't want the rotation to matter\n",
    "* Translation invariance: we don't want the location to matter\n",
    "\n",
    "\n",
    "* To make your CNN rotation and translation invariant, augment the training set with random rotated and translated images\n",
    "* Helps with over-fitting\n",
    "\n",
    "## Groundbreaking CNN Architectures\n",
    "\n",
    "* AlexNet Architecture\n",
    "    * Trained network in about a week\n",
    "    * Pioneered used of ReLU activation and dropout technique for overfitting\n",
    "* VGG Architecture\n",
    "    * VGG-16 and VGG-19 (number of layers)\n",
    "    * Long sequence of 3x3 convoultions broken up by 2x2 pooling layers, finished with 3 fully connected layers\n",
    "* ResNet Architecture\n",
    "\n",
    "All of these are available in Keras, along with some others\n",
    "\n",
    "## Visualizing CNNs\n",
    "\n",
    "See https://classroom.udacity.com/nanodegrees/nd009/parts/99115afc-e849-48cf-a580-cb22eea2ba1b/modules/777db663-2b0d-4040-9ae4-bf8c6ab8f157/lessons/52fc79a7-13ff-4065-b3c6-8203ec9ef60c/concepts/cbf65dc4-c0b4-44c5-81c6-5997e409cb75\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Involves taking a pre-trained neural network and adapting the neural network to a new, different data set\n",
    "\n",
    "Remove layers that are specific to the dataset, keep the other layers, and add one or more layers, and only train those\n",
    "\n",
    "See [here](https://classroom.udacity.com/nanodegrees/nd009/parts/99115afc-e849-48cf-a580-cb22eea2ba1b/modules/777db663-2b0d-4040-9ae4-bf8c6ab8f157/lessons/52fc79a7-13ff-4065-b3c6-8203ec9ef60c/concepts/8c202ff3-aab5-46c3-8ed1-0154fa7b566b) for more info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
