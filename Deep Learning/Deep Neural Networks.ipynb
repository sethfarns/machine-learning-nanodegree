{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "Perceptron checks inputs times their weights, plus the bias, and checks if it's positive or negative\n",
    "\n",
    "Wx + b >= 0\n",
    "\n",
    "## Why 'Neural Networks'?\n",
    "\n",
    "Neurons in the brain take inputs, perform some calculations on them, and decide to 'fire' or not; similarly, in with deep neural networks, we take weighted inputs, perform calculations, and decide to output a 1 or a 0\n",
    "\n",
    "## Perceptrons as Logical Operators\n",
    "\n",
    "#### AND Perceptron\n",
    "\n",
    "If all inputs are 'true', output is 'true', otherwise output is 'false'\n",
    "\n",
    "#### OR perceptron\n",
    "\n",
    "If any inputs are 'true', output is 'true', otherwise output is 'false'\n",
    "\n",
    "#### XOR perceptron\n",
    "\n",
    "If exactly one input is 1 and one is 0, output 'true', otherwise 'false'\n",
    "\n",
    "## XOR Multi-Layer Perceptron\n",
    "\n",
    "NAND (AND + NOT) and OR, then AND from the output of both to get XOR\n",
    "\n",
    "## Perceptron Trick\n",
    "\n",
    "For moving the line to better classify incorrectly classified points\n",
    "\n",
    "Take the equation for the separating line, subtract (for negative point classified as positive, or add for positive point classified as negative) the incorrectly classified point multiplied by a determined factor (the learning rate), to get the equation for the new separating line\n",
    "\n",
    "## Error Function\n",
    "\n",
    "To use gradient descent, the error function must be continuous and differentiable. In order to do this, we have to do continuous predictions instead of discrete. Just change the step function (0 or 1) to the sigmoid function.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "## Softmax\n",
    "\n",
    "Equivalent of sigmoid activation function, but when the problem has 3 or more classes\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "Maximize probability of all points. Use the natural log of the probabilities rather than the product\n",
    "\n",
    "## Cross Entropy\n",
    "See function below\n",
    "\n",
    "## Multi-class Cross Entropy\n",
    "\n",
    "more complicated...\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Error function\n",
    "\n",
    "If y == 1<br>\n",
    "P(pos) = $\\hat{y}$<br>\n",
    "Error = -ln($\\hat{y}$)\n",
    "\n",
    "If y == 0<br>\n",
    "P(neg) = 1 - P(pos) = 1 - $\\hat{y}$<br>\n",
    "Error = -ln(1 - $\\hat{y}$)\n",
    "\n",
    "Summarized into one function:\n",
    "$$Error = -(1-y)(ln(1-\\hat{y})) - yln(\\hat{y})$$\n",
    "\n",
    "$$ Error function = -\\frac{1}{m}\\sum_{i=1}^{m} (1-y_i)(ln(1-\\hat{y_i})) + y_{i}ln(\\hat{y_i}) $$\n",
    "\n",
    "$$ Multiclass error function = -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}y_{ij}ln(\\hat{y}_{ij}) $$\n",
    "\n",
    "$$ E(W,b) = -\\frac{1}{m}\\sum_{i=1}^{m} (1-y_i)(ln(1-\\sigma(Wx^{(i)}+b))) + y_{i}ln(\\sigma(Wx^{(i)}+b)) $$\n",
    "\n",
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "Gradient descent: Change $w_i$ to $$ w_{i} + a(y-\\hat{y})x_{i} $$\n",
    "\n",
    "Perceptron algorithm: If x is misclassified, change $w_i$ to $w_i$ + a x$_i$ if positive, or\n",
    "$$ w_{i} - a x_{i} $$ if negative\n",
    "\n",
    "\n",
    "## Non-linear Models\n",
    "\n",
    "* Combine multiple linear models into a non-linear model\n",
    "* Take probability of each point being positive or negative for each model\n",
    "* Combine these probabilities by adding them together and then applying the sigmoid function\n",
    "* Can add weights to each probability to make one model more important than the others\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "* Create deep learning network by combining linear models to create non-linear models, then combining those to create more non-linear models\n",
    "    * This is where the magic of deep learning happens!\n",
    "* Input layer: weights\n",
    "* Hidden layer: where linear models are combined\n",
    "* Output layer: final non-linear model\n",
    "* One output node for each class (for multi-class classification), with each output being the probability that the point lies in that class\n",
    "\n",
    "## Feedforward\n",
    "\n",
    "* Process of multiplying weights by matrices (for each layer) with the sigmoid function until we get the final prediction\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "* Do feedforward operation\n",
    "* Compare out of model with desired output\n",
    "* Calculate error\n",
    "* Run the feedforward operation backwards (backpropagation) to spread the error to each of the weights\n",
    "* Use this to update the weights, and get a better model\n",
    "* Continue this until we have a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_combiner(w1, w1_weight, w2, w2_weight, b):\n",
    "    return sigmoid_function(w1 * w1_weight + w2 * w2_weight + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168273035060777\n",
      "0.8807970779778823\n",
      "0.8021838885585818\n"
     ]
    }
   ],
   "source": [
    "print(perceptron_combiner(2, 0.4, 6, 0.6, -2))\n",
    "print(perceptron_combiner(3, 0.4, 5, 0.6, -2.2))\n",
    "print(perceptron_combiner(5, 0.4, 4, 0.6, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1 * math.log(0.7 * 0.9 * 0.8 * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cross_entropy([1, 1, 0], [0.8, 0.7, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log\n",
    "\n",
    "# minimize the cross entropy to improve model   \n",
    "def binary_cross_entropy(labels, probabilities):\n",
    "    return -1 * sum([(y*log(p)) + ((1-y)*log(1-p)) for p,y in zip(probabilities, labels)])\n",
    "\n",
    "def softmax(L):\n",
    "    total = sum([np.exp(x) for x in L])\n",
    "    return [np.exp(x) / total for x in L]\n",
    "\n",
    "def perceptron_trick(current_line, point, learning_rate=0.1, current_positive=True):\n",
    "    if current_positive:\n",
    "        new_line = [round(x - (learning_rate*y), 2) for x,y in zip(current_line, point + [1])]\n",
    "    else:\n",
    "        new_line = [round(x + (learning_rate*y), 2) for x,y in zip(current_line, point + [1])]\n",
    "    return new_line\n",
    "\n",
    "def sigmoid_function(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def step_function(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron_classify(equation, point, function='sigmoid'):\n",
    "    computed = [x * y for x, y in zip(equation, point + [1])]\n",
    "    if function == 'sigmoid':\n",
    "        return sigmoid_function(sum(computed))\n",
    "    else:\n",
    "        return step_function(sum(computed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.8807970779778823], [0, 0.11920292202211755]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_1 = []\n",
    "equation_1 = [1, 1, 0]\n",
    "points = [[1,1], [-1,-1]]\n",
    "\n",
    "for point in points:\n",
    "    pred = perceptron_classify(equation_1, point)\n",
    "    if pred > 0.5:\n",
    "        preds_1.append([1, pred])\n",
    "    else:\n",
    "        preds_1.append([0, pred])\n",
    "preds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.9999999979388463], [0, 2.0611536181902037e-09]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_2 = []\n",
    "equation_2 = [10, 10, 0]\n",
    "points = [[1,1], [-1,-1]]\n",
    "\n",
    "for point in points:\n",
    "    pred = perceptron_classify(equation_2, point)\n",
    "    if pred > 0.5:\n",
    "        preds_2.append([1, pred])\n",
    "    else:\n",
    "        preds_2.append([0, pred])\n",
    "preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_1 = binary_cross_entropy([x for x,y in preds_1], [y for x,y in preds_1])\n",
    "error_2 = binary_cross_entropy([x for x,y in preds_2], [y for x,y in preds_2])\n",
    "\n",
    "error_1 > error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999979388463"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_classify([10, 10, 0], [1, 1])\n",
    "\n",
    "perceptron_classify([10, 10, 0], [-1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = [3, 4, -10]\n",
    "point = [1, 1]\n",
    "lr = 0.1\n",
    "cp = False\n",
    "counter = 0\n",
    "while perceptron_classify(line, point) == 0:\n",
    "    line = perceptron_trick(line, point, lr, cp)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "41.5 + 38.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def and_perceptron(x1, x2):\n",
    "    weight = 1\n",
    "    bias = -1.5\n",
    "    weighted_inputs = [weight * x for x in [x1, x2]]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_perceptron(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def or_perceptron(x1, x2):\n",
    "    weight = 1\n",
    "    bias = -0.5\n",
    "    weighted_inputs = [weight * x for x in [x1, x2]]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_perceptron(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def not_perceptron(x1, x2):\n",
    "    w1 = 0.0\n",
    "    w2 = -1.0\n",
    "    bias = 0.5\n",
    "    weighted_inputs = [w * x for w, x in zip([w1, w2], [x1, x2])]\n",
    "    if sum(weighted_inputs) + bias >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_perceptron(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xor_perceptron(x1, x2):\n",
    "    if or_perceptron(x1, x2) == 1 and and_perceptron(x1, x2) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_perceptron(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keras.models.Sequential` class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like `compile()`, `fit()`, and `evaluate()` that are used to train and run the model. We'll cover these functions soon, but first let's start looking at the layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras requires the input shape to be specified in the first layer, but it will automatically infer the shape of all other layers. This means you only have to explicitly set the input dimensions for the first layer.\n",
    "\n",
    "The first (hidden) layer from above, `model.add(Dense(32, input_dim=X.shape[1]))`, creates 32 nodes which each expect to receive 2-element vectors as inputs. Each layer takes the outputs from the previous layer as inputs and pipes through to the next layer. This chain of passing output to the next layer continues until the last layer, which is the output of the model. We can see that the output has dimension 1.\n",
    "\n",
    "The activation \"layers\" in Keras are equivalent to specifying an activation function in the Dense layers (e.g., `model.add(Dense(128))`; `model.add(Activation('softmax'))` is computationally equivalent to `model.add(Dense(128, activation=\"softmax\"))`), but it is common to explicitly separate the activation layers because it allows direct access to the outputs of each layer before the activation is applied (which is useful in some model architectures).\n",
    "\n",
    "Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data. We'll specify the loss function to be `categorical_crossentropy` which can be used when there are only two classes, and specify `adam` as the optimizer (which is a reasonable default when speed is a priority). And finally, we can specify what metrics we want to evaluate the model with. Here we'll use accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained with the `fit()` method, through the following command that specifies the number of training epochs and the message level (how much information we want displayed on the screen during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Predictions:\n",
      "[[ 0.42938578]\n",
      " [ 0.55750144]\n",
      " [ 0.53029436]\n",
      " [ 0.45875055]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Using TensorFlow 1.0.0; use tf.python_io in later versions\n",
    "tf.python_io.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "\n",
    "# Add required layers\n",
    "xor.add(Dense(8, input_dim=2))\n",
    "xor.add(Activation('tanh'))\n",
    "xor.add(Dense(1))\n",
    "xor.add(Activation('sigmoid'))\n",
    "\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "xor.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, epochs=250, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Optimization\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent\n",
    "Stochastic gradient descent is very easy to implement in Keras. All we need to do is specify the size of the batches in the training process, as follows:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=100, verbose=0)\n",
    "```\n",
    "\n",
    "### Learning Rate Decay\n",
    "\n",
    "In general, better to have a small learning rate (learns better, but slower)\n",
    "A decreasing learning rate is one that gets smaller as the model gets closer to the optimal solution\n",
    "\n",
    "### Testing in Keras\n",
    "\n",
    "In order to test in Keras, all we need to do is to split our set into a training and testing sets. Since we have 400 data points, using 50 for training makes sense:\n",
    "```python\n",
    "(X_train, X_test) = X[50:], X[:50]\n",
    "(y_train, y_test) = y[50:], y[:50]\n",
    "```\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Stop when testing error starts to increase to avoid over-fitting\n",
    "\n",
    "### Regularization\n",
    "\n",
    "\"The whole problem with AI is that bad models are so certain of themselves, and good models so full of doubts.\"\n",
    "\n",
    "Large coefficients -> overfitting\n",
    "Penalize large weights\n",
    "L1 error function/regularization: good for feature selection\n",
    "L2 error function/regularization: normally better for training models\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Sometimes one part of the network dominates; to solve this, randomly turn off some nodes to leverage all of the nodes\n",
    "Dropout parameter is probability each node will be dropped\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "With sigmoid function, derivative of points far to the right or left are almost 0, which means that the step for gradient descent will be very small\n",
    "With gradient descent and learning decay, we never make it to the optimal minimum if the steps are too small\n",
    "Best way to change this is to change the activation function\n",
    "\n",
    "### Other Activation Functions\n",
    "\n",
    "Hyperbolic tangent functions; similar to sigmoid but derivates are large\n",
    "Rectified linear unit (ReLU); very simple function, basically the max between x and 0\n",
    "\n",
    "### Local Minima\n",
    "\n",
    "#### Random restart\n",
    "\n",
    "Use random restarts to start from a few different, random places, and do gradient descent from all of them, to increase the chances of arriving at the optimal minimum\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "Use momentum to get 'over the hump', by taking the average of the last few steps, and weight them with a recency bias\n",
    "\n",
    "### Keras Optimizers\n",
    "\n",
    "#### SGD\n",
    "\n",
    "This is Stochastic Gradient Descent. It uses the following parameters:\n",
    "\n",
    "* Learning rate.\n",
    "* Momentum (This takes the weighted average of the previous steps, in order to get a bit of momentum and go over bumps, as a way to not get stuck in local minima).\n",
    "* Nesterov Momentum (This slows down the gradient when it's close to the solution).\n",
    "\n",
    "#### Adam\n",
    "\n",
    "Adam (Adaptive Moment Estimation) uses a more complicated exponential decay that consists of not just considering the average (first moment), but also the variance (second moment) of the previous steps.\n",
    "\n",
    "#### RMSProp\n",
    "\n",
    "RMSProp (RMS stands for Root Mean Squared Error) decreases the learning rate by dividing it by an exponentially decaying average of squared gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
